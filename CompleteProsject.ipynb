{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING YARDAGE FOR THE NFL\n",
    "\n",
    "\n",
    "## 1. BIG PICTURE\n",
    "\n",
    "The goal in American football is for the offence to run (rush) or throw (pass) the ball to gain yards, move towards and finally across the opposing team's side of the field to score. The defenses' goal is to prevent the offensive team from scoring. \n",
    "\n",
    "Her eI use machine learning to find context to what contributes to a successfull run play, and predict the yardage (how many yards the team will gain on a rushing play). \n",
    "\n",
    "The data is from the Superbowl season 2017-2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GET THE DATA\n",
    "\n",
    "The data was downloaded from https://www.kaggle.com/c/nfl-big-data-bowl-2020/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "from   datetime          import date\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv', low_memory=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out what needed to be done to improve the data I examined whether the features were numerical or not, and how many `NaN` values there were.\n",
    "\n",
    "I then examined in much more detail the features that were Strings and the ones with `NaN` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 509762 entries, 0 to 509761\n",
      "Data columns (total 49 columns):\n",
      "GameId                    509762 non-null int64\n",
      "PlayId                    509762 non-null int64\n",
      "Team                      509762 non-null object\n",
      "X                         509762 non-null float64\n",
      "Y                         509762 non-null float64\n",
      "S                         509762 non-null float64\n",
      "A                         509762 non-null float64\n",
      "Dis                       509762 non-null float64\n",
      "Orientation               509744 non-null float64\n",
      "Dir                       509748 non-null float64\n",
      "NflId                     509762 non-null int64\n",
      "DisplayName               509762 non-null object\n",
      "JerseyNumber              509762 non-null int64\n",
      "Season                    509762 non-null int64\n",
      "YardLine                  509762 non-null int64\n",
      "Quarter                   509762 non-null int64\n",
      "GameClock                 509762 non-null object\n",
      "PossessionTeam            509762 non-null object\n",
      "Down                      509762 non-null int64\n",
      "Distance                  509762 non-null int64\n",
      "FieldPosition             503338 non-null object\n",
      "HomeScoreBeforePlay       509762 non-null int64\n",
      "VisitorScoreBeforePlay    509762 non-null int64\n",
      "NflIdRusher               509762 non-null int64\n",
      "OffenseFormation          509652 non-null object\n",
      "OffensePersonnel          509762 non-null object\n",
      "DefendersInTheBox         509696 non-null float64\n",
      "DefensePersonnel          509762 non-null object\n",
      "PlayDirection             509762 non-null object\n",
      "TimeHandoff               509762 non-null object\n",
      "TimeSnap                  509762 non-null object\n",
      "Yards                     509762 non-null int64\n",
      "PlayerHeight              509762 non-null object\n",
      "PlayerWeight              509762 non-null int64\n",
      "PlayerBirthDate           509762 non-null object\n",
      "PlayerCollegeName         509762 non-null object\n",
      "Position                  509762 non-null object\n",
      "HomeTeamAbbr              509762 non-null object\n",
      "VisitorTeamAbbr           509762 non-null object\n",
      "Week                      509762 non-null int64\n",
      "Stadium                   509762 non-null object\n",
      "Location                  509762 non-null object\n",
      "StadiumType               476828 non-null object\n",
      "Turf                      509762 non-null object\n",
      "GameWeather               466114 non-null object\n",
      "Temperature               461230 non-null float64\n",
      "Humidity                  503602 non-null float64\n",
      "WindSpeed                 442332 non-null object\n",
      "WindDirection             429528 non-null object\n",
      "dtypes: float64(10), int64(15), object(24)\n",
      "memory usage: 190.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PROBLEMS IN THE DATA AND PROCESSING THEM:\n",
    "\n",
    "After examining the data I found several things that needed to be fixed. I will not go into all the examining here because it was very extensive and quite messy.\n",
    "\n",
    "However I have here a summary of the different features I needed to change and a short description of what needed to be done:\n",
    "\n",
    "#### TIMESTAMPS\n",
    "The feature `GameClock` was given on the format `hh:mm:ss` as a string and I wanted to use numerical information for this. The solution was to transform the timestamp into three new features: `GameClockHour`, `GameClockMinute` and `GameClockSecond`. For this I used the function `timeStampSplit`.\n",
    "\n",
    "Then I delete the original feature `GameClock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeStampSplit(df, col):\n",
    "    \"\"\"\n",
    "    Function that takes a timestamp on the format hh:mm:ss as and splits it into new columns\n",
    "    for the hour, minute and seconds.\n",
    "    \n",
    "    Input:\n",
    "        df  = The original dataframe\n",
    "        col = The column name for the feature we want to split\n",
    "        \n",
    "    Output:\n",
    "        An updated dataframe.\n",
    "    \"\"\"\n",
    "    # Define the new column names:\n",
    "    colNames = [col + 'Hour', col + 'Minute', col + 'Second'];\n",
    "    # Create a new dataframe, the one we want to add:\n",
    "    new_df = pd.DataFrame(df[col].str.split(':', 2).tolist(), columns=colNames);\n",
    "    \n",
    "    # Make the new features numerical:\n",
    "    for c in colNames:\n",
    "        new_df[c] = pd.to_numeric(new_df[c]);\n",
    "    \n",
    "    # Concat the two dataframes and return it:\n",
    "    return pd.concat([df, new_df], axis=1, sort=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HEIGHT\n",
    "\n",
    "The player heights are given in the feature `PlayerHeight` on the format `foot-inches`. To make this into a numerical value I created the function `heightToCm` which also transforms the height into centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heightToCm(str):\n",
    "    \"\"\"\n",
    "    Function used to convert the player heights given on the format foot-inches into cm.\n",
    "    \n",
    "    Input: \n",
    "        str = the height given as foot-inches, e.g. 5-10\n",
    "        \n",
    "    Output:\n",
    "        The height now in centimeteres, as a float\n",
    "        \n",
    "    Usage:\n",
    "        Here it is used on the PlayerHeight feature.\n",
    "    \"\"\"\n",
    "    if pd.isnull(str):\n",
    "        return np.nan;\n",
    "    # Separate the foot and inches:\n",
    "    arr = str.split('-');\n",
    "    \n",
    "    height = int(arr[0])*30.48 + int(arr[1])*2.54;\n",
    "    return height;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates\n",
    "\n",
    "For some features we have dates as information. For these we want to split the strings into year, month and day features. This is relevant for the feature `PlayerBirthDate`. But also for `TimeHandoff` and `TimeSnap` which are given as UTC time stamps. For these we use the function `utcSplit` which uses both `timeStampSplit` and `dateSplit`. \n",
    "\n",
    "Then we remove the features `PlayerBirthDate`, `TimeHandoff` and `TimeSnap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateSplit(df, col, char):\n",
    "    \"\"\"\n",
    "    Functions which take the date-stamp and split it into three new features for the year,\n",
    "    month and day. The dates can be given on two formats:\n",
    "        mm/dd/yyyy or yyyy-mm-dd\n",
    "    \n",
    "    Input:\n",
    "        df   = the original dataframe\n",
    "        col  = the column name of the dataframe we want to split\n",
    "        char = the character used to split between the year, month and day\n",
    "        \n",
    "    Output:\n",
    "        An updated dataframe\n",
    "    \"\"\"\n",
    "    # The column names depends on the character used for the split:\n",
    "    if char == '-':\n",
    "        colNames = [col + 'Year', col + 'Month', col + 'Day'];\n",
    "    else:\n",
    "        colNames = [col + 'Month', col + 'Day', col + 'Year'];\n",
    "        \n",
    "    # Create a new dataframe, the one we want to add:\n",
    "    new_df = pd.DataFrame(df[col].str.split(char, 2).tolist(), columns=colNames);\n",
    "    \n",
    "    # Make the new features numerical:\n",
    "    for c in colNames:\n",
    "        new_df[c] = pd.to_numeric(new_df[c]);\n",
    "    \n",
    "    # Concat the two dataframes and return it:\n",
    "    return pd.concat([df, new_df], axis=1, sort=False);\n",
    "\n",
    "def utcSplit(df, col):\n",
    "    \"\"\"\n",
    "    Function to split the UTC timestamp into new features for the year, month, day, hour,\n",
    "    minute and seconds.\n",
    "    \n",
    "    Input:\n",
    "        df  = the original dataframe\n",
    "        col = the column we want to apply this on\n",
    "    \n",
    "    Output:\n",
    "        An updated dataframe\n",
    "    \"\"\"\n",
    "    # Create a temporary dataframe copy to separate the date and time:\n",
    "    df1      = df[col].copy();\n",
    "    # Remove the timestamp:\n",
    "    df1[col] = df[col].apply(lambda s: s[:s.find('T')]);\n",
    "    # Split into the new features:\n",
    "    df1      = dateSplit(df1, col, '-');\n",
    "    # Remove the original column from this one:\n",
    "    df1      = df1.drop(col, axis=1);\n",
    "    \n",
    "    # Create another temporary dataframe copy to separate the date and time:\n",
    "    df2      = df[col].copy();\n",
    "    # Remove the date stamp:\n",
    "    df2[col] = df[col].apply(lambda s: s[s.find('T')+1:-5]);\n",
    "    # Split into the new features:\n",
    "    df2      = timeStampSplit(df2, col)\n",
    "    # Remove the original column:\n",
    "    df2      = df2.drop(col, axis=1);\n",
    "    \n",
    "    # Concat all the new features:\n",
    "    df3 = pd.concat([df1, df2], axis=1, sort=False);\n",
    "    \n",
    "    # Concat with the original and return:\n",
    "    return pd.concat([df, df3], axis=1, sort=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOCATION\n",
    "\n",
    "The `Location` feature in the data tells us the location of the game. This feature is riddled with writing errors and information in different formats. To handle this I examined each of the locations and found that the best format is simply the city name in all caps. \n",
    "\n",
    "This feature is handled using the `fixLocation` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_locations = [];\n",
    "def fixLocation(loc):\n",
    "    \"\"\"\n",
    "    Function to fix the locations so they just describe the city of the game.\n",
    "    \n",
    "    Input:\n",
    "        loc = the current location\n",
    "        \n",
    "    Output:\n",
    "        Correct format for the current location\n",
    "    \"\"\"\n",
    "    old_loc = loc;\n",
    "    if pd.isnull(loc):\n",
    "        return np.nan;\n",
    "    \n",
    "    # First we remove the ',' from the location:\n",
    "    if loc.find(',') != -1:\n",
    "        loc = loc[:loc.find(',')];\n",
    "    \n",
    "    # Then if the , has been confused with a .:\n",
    "    if loc.find('.') != -1 and len(loc[loc.find('.'):]) < 5:\n",
    "        loc = loc[:loc.find('.')];\n",
    "        \n",
    "    # The last option is to see if we find the correct version in the already\n",
    "    # edited locations:\n",
    "    if loc.find(' ') != -1:\n",
    "        words = loc.split(' ');\n",
    "        word  = words[0]\n",
    "        if len(word) < 3: \n",
    "            word = words[1];\n",
    "        if word in str(edited_locations):\n",
    "            for l in edited_locations:\n",
    "                if word in l:\n",
    "                    loc = l;\n",
    "                    break;\n",
    "        elif len(words) > 2 and len(words[2]) < 3:\n",
    "            loc = words[0] + words[1];\n",
    "                    \n",
    "    if loc not in edited_locations:\n",
    "        edited_locations.append(loc);\n",
    "    \n",
    "    return loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STADIUM NAMES\n",
    "\n",
    "Because the `StadiumType` depens on the Stadium name we need to fix this first. Here the main problem is wrong or outdated names on the stadiums. The correct ones were found through googling.\n",
    "\n",
    "Here names are corrected so they are all in the same format, and updated if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixStadiums(stadium):\n",
    "    \"\"\"\n",
    "    Function to change the stadion names to be the correct format, and to update the names if they have changed.\n",
    "    The source for changing the names was wikipedia.\n",
    "    \n",
    "    Input:\n",
    "        stadium = The stadium name\n",
    "        \n",
    "    Output:\n",
    "        The correct name and format for the stadium\n",
    "    \"\"\"\n",
    "    if pd.isnull(stadium):\n",
    "        return np.nan;\n",
    "    \n",
    "    s = \"STADIUM\";\n",
    "    # Handle the misspellings:\n",
    "    if stadium.endswith(\"STDIUM\"):\n",
    "        stadium = stadium.replace(\"STDIUM\", s);\n",
    "    elif stadium.endswith(\"COLIESUM\"):\n",
    "        stadium = stadium.replace(\"COLIESUM\", \"COLISEUM\");\n",
    "    \n",
    "    # Fix everything for firstenergy:\n",
    "    if stadium.find(\"FIRST\") != -1:\n",
    "        stadium = \"FIRSTENERGY \" + s;\n",
    "    # Fix every M&T:\n",
    "    if stadium.find(\"&\") != -1 and stadium[stadium.find(\"&\")] != 'T':\n",
    "        stadium = \"M&T BANK \" + s;\n",
    "    # Fix all the Mercedes-Benz:\n",
    "    if stadium.find(\"MERCEDES\") != -1:\n",
    "        stadium = \"MERCEDES-BENZ SUPERDOME\";\n",
    "    # Fix all the CenturyLink:\n",
    "    if stadium.find(\"CENTURY\") != -1:\n",
    "        stadium = \"CENTURYLINK FIELD\";\n",
    "    # Fix all the MetLife:\n",
    "    if stadium.find(\"METLIFE\") != -1:\n",
    "        stadium = \"METLIFE \" + s;  \n",
    "    # Fix all the Twickenham:\n",
    "    if stadium.find(\"TWICKENHAM\") != -1:\n",
    "        stadium = \"TWICKENHAM \" + s;\n",
    "    \n",
    "    # Update the wrong ones:\n",
    "    if stadium.find(\"UNIVERSITY\") != -1:\n",
    "        stadium = \"STATE FARM \" + s;\n",
    "    elif stadium.find(\"MILE\") != -1:\n",
    "        stadium = \"EMPOWER FIELD\";\n",
    "    elif stadium.find(\"EVERBANK\") != -1:\n",
    "        stadium = stadium.replace(\"EVERBANK\", \"TIAA BANK\"); \n",
    "    elif stadium.find(\"STUB\") != -1:\n",
    "        stadium = \"DIGNITY HEALTH SPORTS PARK\";    \n",
    "    elif stadium.find(\"OAKLAND\") != -1:\n",
    "        stadium = \"RINGCENTRAL COLISEUM\";\n",
    "        \n",
    "    return stadium;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STADIUM TYPE\n",
    "\n",
    "For the `StadiumType` feature there were a lot of writing mistakes and different definitions which all could simply be sorted into Indoors or Outdoors. To fix this each element was sorted as either Indoors, and then assigned the value `1` or outdoors with the assigned value `0`. \n",
    "\n",
    "The features `StadiumType` and `Stadium` are obviously related, and we also check if the `Stadium` already has a defined `StadiumType` value for the `NaN` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we write the function we also need to define where the StadiumType is nan, \n",
    "# And the corresponding stadiums:\n",
    "nanStadiums     = train_df.Stadium[pd.isnull(train_df.StadiumType)].unique();\n",
    "nanStadiumTypes = [];\n",
    "for nsta in nanStadiums:\n",
    "    if len(train_df.StadiumType[train_df.Stadium == nsta].value_counts()) == 0:\n",
    "        nanStadiumTypes.append(train_df.StadiumType.iloc[train_df.StadiumType[train_df.Stadium == nanStadiums[0]].index[0]]);\n",
    "    else:\n",
    "        nanStadiumTypes.append(train_df.StadiumType[train_df.Stadium == nsta].value_counts().idxmax())\n",
    "\n",
    "        \n",
    "def fixStadiumType(stad):\n",
    "    \"\"\"\n",
    "    Function to sort the stadium type in either outdoors = 0, or indoors = 1. Also\n",
    "    replaces the nan values as much as possible based on the stadium name.\n",
    "    \n",
    "    Input:\n",
    "        stad = the stadium type\n",
    "        \n",
    "    Output:\n",
    "        0 or 1, depending on the sorting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(stad):\n",
    "        return np.nan;\n",
    "    \n",
    "    sType = stad[0];\n",
    "    sName = stad[1];\n",
    "    \n",
    "    \n",
    "    if pd.isnull(sType):\n",
    "        ind   = np.where(nanStadiums == sName)[0][0];\n",
    "        sType = nanStadiumTypes[ind];\n",
    "    \n",
    "    if not pd.isnull(sType):\n",
    "        if sType.find('Ou') != -1 or sType.find('Open') != -1 or sType.find('open') != -1 or \\\n",
    "        sType.find('Field') != -1 or sType.find('Cloudy') != -1 or sType.find('Bowl') != -1:\n",
    "            return 0;\n",
    "        else:\n",
    "            return 1;\n",
    "        \n",
    "    return sType;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TURF\n",
    "\n",
    "The turf is the type of grass the field has and can be either Natural, Artificial or a Hybrid. For the `Turf` feature I wrote a function to sort the different types into these three alternatives, after doing some research on the different types of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixTurf(t):\n",
    "    \"\"\"\n",
    "    Function for sorting the different types of turf into Artificial, Natural and Hybrid.\n",
    "    \n",
    "    Input:\n",
    "        t = the turf definition\n",
    "        \n",
    "    Output: \n",
    "        Artificial/Hybrid/Natural\n",
    "        \n",
    "    Usage:\n",
    "        For use on the Turf feature.\n",
    "    \"\"\"\n",
    "    if pd.isnull(t):\n",
    "        return np.nan;\n",
    "    \n",
    "    \n",
    "    if t.find('Turf') != -1 or t.find('turf') != -1 or t.find('Artif') != -1 or t.find('UBU') != -1:\n",
    "        t = \"Artificial\";\n",
    "    elif t.find('DD') != -1 or t.find(\"SIS\") != -1:\n",
    "        t = \"Hybrid\";\n",
    "    else:\n",
    "        t = \"Natural\";\n",
    "    \n",
    "    return t;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WEATHER\n",
    "\n",
    "The `GameWeather` feature has many different descriptions, but can for the most part be sorted into 6 categories: indoors, sunny, rain, snow, fog and cloudy. For the missing values we estimate the category based on the `Humidity` and `Temperature` features. \n",
    "\n",
    "NB: have to estimate median and mean values for these two features before running this function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# First we fix so the weather indoors is indoors:\n",
    "train_df.GameWeather[train_df.StadiumType == 1] = \"INDOORS\";\n",
    "\n",
    "def fixWeather(w):\n",
    "    \"\"\"\n",
    "    Function sorting the different weather conditions into the following categories:\n",
    "        INDOORS\n",
    "        SUNNY\n",
    "        RAIN\n",
    "        SNOW\n",
    "        FOG\n",
    "        CLOUDY\n",
    "        \n",
    "    Quite roughly sorted, the definitions were very different considering details.\n",
    "    \n",
    "    Input:\n",
    "        w = The weather definition\n",
    "        \n",
    "    Output: \n",
    "        The sorting\n",
    "        \n",
    "    Usage:\n",
    "        For the GameWeather feature.\n",
    "    \"\"\"\n",
    "    if isinstance(w, float):\n",
    "        return '';\n",
    "    \n",
    "    if w.find('INDO') != -1 or w.find('CONTROLLED') != -1 or w.find('T: 51') != -1:\n",
    "        w = 'INDOORS';\n",
    "    elif w.find('SUN') != -1 or w.find('CLEAR') != -1 or w.find('FAIR') != -1:\n",
    "        w = 'SUNNY';\n",
    "    elif w.find('RAIN') != -1 or w.find('SHOWE') != -1:\n",
    "        w = \"RAIN\";\n",
    "    elif w.find('SNOW') != -1:\n",
    "        w = \"SNOW\";\n",
    "    elif w.find('FOG') != -1 or w.find('HAZY') != -1:\n",
    "        w = \"FOG\";\n",
    "    else:\n",
    "        w = \"CLOUDY\";\n",
    "    \n",
    "    return w;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on also adjust for the `NaN` values based on the `Temperature` and `Humidity` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "allNanvalues = [train_df[\"Humidity\"][pd.isnull(train_df[\"GameWeather\"])].unique(), \\\n",
    "            train_df[\"Temperature\"][pd.isnull(train_df[\"GameWeather\"])].unique()]\n",
    "\n",
    "for nanvalues in allNanvalues:\n",
    "    for val in nanvalues:\n",
    "        if len(train_df[\"GameWeather\"][train_df[\"Humidity\"] == val].value_counts()) > 0:\n",
    "            gwHumid = train_df[\"GameWeather\"][train_df[\"Humidity\"] == val].value_counts().idxmax();\n",
    "            temp    = train_df[\"Temperature\"][train_df[\"Humidity\"] == val].value_counts().idxmax();\n",
    "            gwTemp  = train_df[\"GameWeather\"][train_df[\"Temperature\"] == temp].value_counts().idxmax();\n",
    "            if gwHumid == gwTemp:\n",
    "                inds = train_df[\"GameWeather\"][(pd.isnull(train_df[\"GameWeather\"])) & (train_df[\"Humidity\"] == val)];\n",
    "                train_df[\"GameWeather\"][inds.index] = gwHumid;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEAM\n",
    "\n",
    "The `Team` feature can be set to either home or away. To make it numerical I created a function to set the the value to `0` if the team is `away` and `1` if the team is `home`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixTeam(t):\n",
    "    \"\"\"\n",
    "    Function to change away/home into numbers. Chose to use 0 = away and 1 = home.\n",
    "    \n",
    "    Input:\n",
    "        t = away or home\n",
    "        \n",
    "    Output:\n",
    "        0 or 1\n",
    "        \n",
    "    Usage:\n",
    "        For the Team feature.\n",
    "    \"\"\"\n",
    "    if t == 'away':\n",
    "        return 0;\n",
    "    else:\n",
    "        return 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WIND\n",
    "\n",
    "I created two functions for the two features `WindSpeed` and `WindDirection` because for some of the cases the values were switched between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirInSpeed = ['SSW', 'E', 'CALM', 'SE'];\n",
    "speedInDir = ['13', '8', 'CALM', '1'];\n",
    "# Function to check if the value can be converted to an int:\n",
    "def isInt(value):\n",
    "    try:\n",
    "        float(value);\n",
    "        return True;\n",
    "    except ValueError:\n",
    "        return False;\n",
    "\n",
    "def fixWindSpeed(ws):\n",
    "    \"\"\"\n",
    "    Fixes the windspeed, takes the values and turns them into floats. Also removes all instances of units [mph],\n",
    "    converts from gusts and estimates a mean if there are two values given.\n",
    "    \n",
    "    Also switches the instances that have been mixes with the wind direction.\n",
    "    \n",
    "    Input: \n",
    "        ws = the original windspeed value on the different formats\n",
    "        \n",
    "    Output:\n",
    "        The numerical value for the windspeed\n",
    "        \n",
    "    Usage:\n",
    "        On the feature WindSpeed.\n",
    "    \"\"\"\n",
    "    if isinstance(ws, float):\n",
    "        return ws;\n",
    "    if isInt(ws):\n",
    "        ws = float(ws);\n",
    "    else:\n",
    "        if (ws in dirInSpeed):\n",
    "            replacement = train_df.WindDirection[train_df.WindSpeed == ws].iloc[0];\n",
    "            if isInt(replacement):\n",
    "                ws = float(replacement);\n",
    "            else:\n",
    "                ws = replacement;\n",
    "        elif ws.find(\"MPH\") != -1: # For e.g. 13MPH\n",
    "            ws = float(ws.replace(\"MPH\", \"\"));\n",
    "        elif ws.find(\"-\") != -1: # For e.g. 12-20\n",
    "            val1 = float(ws[0:ws.find(\"-\")]);\n",
    "            val2 = float(ws[ws.find(\"-\")+1:]);\n",
    "            ws   = (val1 + val2) / 2;\n",
    "        else:\n",
    "            sum   = 0;\n",
    "            count = 0;\n",
    "            for val in ws.split(\" \"):\n",
    "                if isInt(val):\n",
    "                    sum = sum + float(val);\n",
    "                    count = count + 1;\n",
    "            ws = sum/count;\n",
    "    return ws;\n",
    "\n",
    "def fixWindDir(wd):\n",
    "    \"\"\"\n",
    "    Fixes the wind direction feature to have a regular maximum three letter format, and changes the places where\n",
    "    it was mixed with the wind speeds.\n",
    "    \n",
    "    Input:\n",
    "        wd = the original format for the wind direction\n",
    "        \n",
    "    Output:\n",
    "        The updated correct format for the wind direction\n",
    "        \n",
    "    Usage: \n",
    "        On the feature WindDirection.\n",
    "    \"\"\"\n",
    "    if isinstance(wd, float):\n",
    "        return wd;\n",
    "    \n",
    "    if (wd in speedInDir):\n",
    "        wd = dirInSpeed[speedInDir.index(wd)];\n",
    "    \n",
    "    wd = wd.replace(\"NORTH\", \"N\");\n",
    "    wd = wd.replace(\"SOUTH\", \"S\");\n",
    "    wd = wd.replace(\"EAST\", \"E\");\n",
    "    wd = wd.replace(\"WEST\", \"W\");\n",
    "    wd = wd.replace(\"FROM\", \"\");\n",
    "    wd = wd.replace(\"-\", \"\");\n",
    "    wd = wd.replace(\" \", \"\");\n",
    "    \n",
    "    return wd.strip();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLAYER DIRECTION\n",
    "\n",
    "The `PlayerDirection` feature is either left or right, so I created a function that sets the feature to `0` if `left` and `1` if `right`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixPlayDir(dir):\n",
    "    \"\"\"\n",
    "    Encodes the player direction so left = 1 and right = 0.\n",
    "    \n",
    "    Input:\n",
    "        dir = left/right\n",
    "        \n",
    "    Output:\n",
    "        1 / 0\n",
    "        \n",
    "    Usage:\n",
    "        On the feature PlayDirection.\n",
    "    \"\"\"\n",
    "    if dir == 'left':\n",
    "        return 0;\n",
    "    else:\n",
    "        return 1; #right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OFFENSEPERSONNEL & DEFENSEPERSONNEL\n",
    "\n",
    "These two features are quite similar so I created a function two turn them into arrays with the placements for the different players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixOffensePersonnel(str):\n",
    "    \"\"\"\n",
    "    Sorting the offense personell into an array according to the format:\n",
    "    [QB, OL, RB, TE, WR, DL, LB, DB] where:\n",
    "        QB = Quarterback\n",
    "        OL = Offensive Lineman\n",
    "        RB = Running-back\n",
    "        TE = Tight End\n",
    "        WR = Wide Receiver\n",
    "        DL = Defensive Lineman\n",
    "        LB = Linebacker\n",
    "        DB = Defensive back\n",
    "        \n",
    "    Input:\n",
    "        str = string with the number of people in the different positions.\n",
    "        \n",
    "    Output:\n",
    "        An array describing the number of people in the different positions.\n",
    "        \n",
    "    Usage: \n",
    "        Used on the feature OffensePersonnel\n",
    "    \"\"\"\n",
    "    if pd.isnull(str):\n",
    "        return np.nan;\n",
    "    \n",
    "    res = [0, 0, 0, 0, 0, 0, 0, 0];\n",
    "    arr = [s.strip() for s in str.split(\",\")];\n",
    "    for e in arr:\n",
    "        if e.find('QB') != -1:\n",
    "            res[0] = int(e[:e.find(' ')])\n",
    "        elif e.find('OL') != -1:\n",
    "            res[1] = int(e[:e.find(' ')])\n",
    "        elif e.find('RB') != -1:\n",
    "            res[2] = int(e[:e.find(' ')])\n",
    "        elif e.find('TE') != -1:\n",
    "            res[3] = int(e[:e.find(' ')])\n",
    "        elif e.find('WR') != -1:\n",
    "            res[4] = int(e[:e.find(' ')])\n",
    "        elif e.find('DL') != -1:\n",
    "            res[5] = int(e[:e.find(' ')])\n",
    "        elif e.find('LB') != -1:\n",
    "            res[6] = int(e[:e.find(' ')])\n",
    "        elif e.find('DB') != -1:\n",
    "            res[7] = int(e[:e.find(' ')])\n",
    "        else:\n",
    "            print(\"HAVE NOT ACCOUNTED FOR: \", e);\n",
    "    \n",
    "    \n",
    "    return res;\n",
    "\n",
    "# METHOD 14: Describing the defense as an array\n",
    "def fixDefensePersonnel(str):\n",
    "    \"\"\"\n",
    "    Converting the defense into an array on the format [DL, LB, DB, OL] where:\n",
    "        DL = Defensive Lineman\n",
    "        LB = Linebacker\n",
    "        DB = Defensive back\n",
    "        OL = Offensive Lineman\n",
    "        \n",
    "    Input:\n",
    "        str = String describing the defense\n",
    "        \n",
    "    Output:\n",
    "        Array describing the defense\n",
    "        \n",
    "    Usage:\n",
    "        On the feature DefensePersonnel\n",
    "    \"\"\"\n",
    "    if pd.isnull(str):\n",
    "        return np.nan;\n",
    "    \n",
    "    res = [0, 0, 0, 0];\n",
    "    arr = [s.strip() for s in str.split(\",\")];\n",
    "    for e in arr:\n",
    "        if e.find('DL') != -1:\n",
    "            res[0] = int(e[:e.find(' ')])\n",
    "        elif e.find('LB') != -1:\n",
    "            res[1] = int(e[:e.find(' ')])\n",
    "        elif e.find('DB') != -1:\n",
    "            res[2] = int(e[:e.find(' ')])\n",
    "        elif e.find('OL') != -1:\n",
    "            res[3] = int(e[:e.find(' ')])\n",
    "        else:\n",
    "            print(\"HAVE NOT ACCOUNTED FOR: \", e);\n",
    "                \n",
    "    return res;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POSSESSIONTEAM & FIELDPOSITION\n",
    "\n",
    "These features contain the current team name that has the field or the possession, so to simplify I created a function to sort them into `1` if the team is the `home` team, and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homeOrAwayTeam(team):\n",
    "    \"\"\"\n",
    "    Checks if the team is the away team or the home team and returns numerical values \n",
    "    accordingly, away = 0, home = 1.\n",
    "    \n",
    "    Input: \n",
    "        team = the abbreviation for the team\n",
    "        \n",
    "    Ouput:\n",
    "        1 / 0 depending on if the team is home or away\n",
    "        \n",
    "    Usage:\n",
    "        On the features PossessionTeam and FieldPosition.\n",
    "    \"\"\"\n",
    "    # team[0] --> the input team\n",
    "    # team[1] --> the home team\n",
    "    diffInInput = ['ARZ', 'BLT', 'CLV', 'HST'];\n",
    "    diffInHome  = ['ARI', 'BAL', 'CLE', 'HOU'];\n",
    "    \n",
    "    if team[0] == team[1]: \n",
    "        return 1;\n",
    "    elif team[0] in diffInInput:\n",
    "        ind = diffInInput.index(team[0]);\n",
    "        if (team[1] == diffInHome[ind]):\n",
    "            return 1;\n",
    "        else:\n",
    "            return 0;\n",
    "    else:\n",
    "        return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OFFENSEFORMATION\n",
    "\n",
    "The `OffenseFormation` has two names for one of the formation, so I created a small funciton to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixOffForm(of):\n",
    "    \"\"\"\n",
    "    Collect the two offense formations with different names.\n",
    "    \n",
    "    Input:\n",
    "        of = the offensive formation\n",
    "        \n",
    "    Output:\n",
    "        The updated offensive formation\n",
    "        \n",
    "    Usage:\n",
    "        On the feature OffenseFormation\n",
    "    \"\"\"\n",
    "    if of == \"ACE\":\n",
    "        return \"SINGLEBACK\";\n",
    "    else:\n",
    "        return of;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXECUTING THE FUNCTIONS\n",
    "\n",
    "Before executing the functions I have to make some of the feature-values all in uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "upperFeatures = [\"Location\", \"GameWeather\", \"Stadium\", \"WindSpeed\", \"WindDirection\"];\n",
    "for uf in upperFeatures:\n",
    "    train_df[uf] = train_df[uf].str.upper();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions I wrote have two different formats, they are either ran on the feature column or with the dataframe as input. Start with the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/api.py:107: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/range.py:465: RuntimeWarning: '<' not supported between instances of 'int' and 'str', sort order is undefined for incomparable objects\n",
      "  return self._int64index.union(other)\n"
     ]
    }
   ],
   "source": [
    "train_df = timeStampSplit(train_df, 'GameClock');\n",
    "train_df = dateSplit(train_df, 'PlayerBirthDate', '/');\n",
    "train_df = utcSplit(train_df, 'TimeSnap');\n",
    "# First we need to remove all the places where TimeHandoff has nan value:\n",
    "train_df = train_df[pd.notnull(train_df.TimeHandoff)];\n",
    "train_df = utcSplit(train_df, 'TimeHandoff');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I run the other functions I remove the features that I have replaced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresToDrop = ['GameClock', 'TimeSnap', 'TimeHandoff', 'PlayerBirthDate'];\n",
    "for feat in featuresToDrop:\n",
    "    train_df = train_df.drop(feat, axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I run through the rest of the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function heightToCm at 0x114943ae8>\n",
      "done!\n",
      "<function fixLocation at 0x1149436a8>\n",
      "done!\n",
      "<function fixStadiumType at 0x114975ea0>\n",
      "done!\n",
      "<function fixTurf at 0x114975f28>\n",
      "done!\n",
      "<function fixWeather at 0x114981048>\n",
      "done!\n",
      "<function fixTeam at 0x114981950>\n",
      "done!\n",
      "<function fixStadiums at 0x114975378>\n",
      "done!\n",
      "<function fixWindSpeed at 0x114981ae8>\n",
      "done!\n",
      "<function fixWindDir at 0x114981ea0>\n",
      "done!\n",
      "<function fixPlayDir at 0x114d09598>\n",
      "done!\n",
      "<function fixOffensePersonnel at 0x114d09510>\n",
      "done!\n",
      "<function fixDefensePersonnel at 0x114d09158>\n",
      "done!\n",
      "<function fixOffForm at 0x114d09f28>\n",
      "done!\n",
      "<function homeOrAwayTeam at 0x114d09ae8>\n",
      "done!\n",
      "TOTALLY DONE!!\n"
     ]
    }
   ],
   "source": [
    "functions = [heightToCm, fixLocation, fixStadiumType, fixTurf, fixWeather, fixTeam, \\\n",
    "            fixStadiums, fixWindSpeed, fixWindDir, fixPlayDir, fixOffensePersonnel, \\\n",
    "            fixDefensePersonnel, fixOffForm, homeOrAwayTeam];\n",
    "features  = [[\"PlayerHeight\"], [\"Location\"], [\"StadiumType\"], [\"Turf\"], [\"GameWeather\"], \\\n",
    "             [\"Team\"], [\"Stadium\"], [\"WindSpeed\"], [\"WindDirection\"], [\"PlayDirection\"], \\\n",
    "             [\"OffensePersonnel\"], [\"DefensePersonnel\"], [\"OffenseFormation\"], \\\n",
    "             [\"PossessionTeam\", \"FieldPosition\"]];\n",
    "\n",
    "for i in range(0, len(functions)):\n",
    "    print(functions[i]);\n",
    "    if i == len(functions)-1:\n",
    "        for feat in features[i]:\n",
    "            train_df[feat] = train_df[[feat, \"HomeTeamAbbr\"]].apply(functions[i], axis=1);\n",
    "        print(\"done!\");\n",
    "    else:\n",
    "        for feat in features[i]:\n",
    "            train_df[feat] = train_df[feat].apply(functions[i]);\n",
    "        print(\"done!\");\n",
    "    \n",
    "print(\"TOTALLY DONE!!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to fix the wind direction for inside to be calm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_df[\"WindDirection\"][(train_df[\"StadiumType\"] == 1) & (pd.isnull(train_df[\"WindDirection\"]))] = \"CALM\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop all the rest of the nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more changes\n",
    "\n",
    "The features `OffensePersonnel` and `DefensePersonnel` are now as arrays, but we want simple numerical values for them all, so we create new features and include the numbers there instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFENSEPERSONNEL\n",
    "newNames1 = ['OP_QB', 'OP_OL', 'OP_RB', 'OP_TE', 'OP_WR', 'OP_DL', 'OP_LB', 'OP_DB'];\n",
    "for n1 in newNames1:\n",
    "        train_df[n1] = 0;\n",
    "        \n",
    "# DEFENSEPERSONNEL\n",
    "newNames2 = ['DP_DL', 'DP_LB', 'DP_DB', 'DP_OL'];\n",
    "for n2 in newNames2:\n",
    "    train_df[n2] = 0;\n",
    "    \n",
    "# SOME RELEVANT FUNCTIONS:\n",
    "def ofStr(str):\n",
    "    return str.replace('[','').replace(']','').split(',');\n",
    "\n",
    "def opDivide(arr, i):\n",
    "    return arr[i];\n",
    "\n",
    "#train_df['OffensePersonnel'] = train_df['OffensePersonnel'].apply(ofStr);\n",
    "#train_df['DefensePersonnel'] = train_df['DefensePersonnel'].apply(ofStr);\n",
    "\n",
    "# Create the new features from OffensePersonnel\n",
    "count1 = 0;\n",
    "for n1 in newNames1:\n",
    "    train_df[n1] = train_df['OffensePersonnel'].apply(opDivide, i=count1);\n",
    "    count1  = count1 + 1;\n",
    "\n",
    "# Create the new features from DefensePersonnel\n",
    "count2 = 0;\n",
    "for n2 in newNames2:\n",
    "    train_df[n2] = train_df['DefensePersonnel'].apply(opDivide, i=count2);\n",
    "    count2  = count2 + 1;\n",
    "    \n",
    "# Remove OffensePersonnel and DefensePersonnel:\n",
    "train_df = train_df.drop('OffensePersonnel', axis=1);\n",
    "train_df = train_df.drop('DefensePersonnel', axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do processing-wise is to make sure that the coordinates are within the field, and encode the season to 0 and 1 to get more reasonable values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.drop(train_df.X[(train_df.X < 0) | (train_df.X > 120)].index);\n",
    "train_df = train_df.drop(train_df.Y[(train_df.Y < 0) | (train_df.Y > 53.33)].index);\n",
    "train_df.Season[train_df.Season == 2017] = 0;\n",
    "train_df.Season[train_df.Season == 2018] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data1/processed.csv', low_memory=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to perform one more drop of the nan values because not all the NaN values were dropped before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally save this datastructure that is fully processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data1/processed.csv', index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SELECTING A MODEL\n",
    "\n",
    "Now that we start with the model choosing and encoding we just upload the `processed.csv` file that contains the correctly processed dataframe. So from this point we import everything anew so we don't have to run through the entire notebook again.\n",
    "\n",
    "Because we import everything again we need to reset the variables in the case where we want to run the entire notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                  as     pd\n",
    "import numpy                   as     np\n",
    "import matplotlib.pyplot       as     plt\n",
    "from   datetime                import date\n",
    "from   sklearn.base            import BaseEstimator, TransformerMixin\n",
    "from   sklearn.preprocessing   import OrdinalEncoder\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.model_selection import StratifiedShuffleSplit\n",
    "from   sklearn.impute          import SimpleImputer\n",
    "from   sklearn.preprocessing   import StandardScaler\n",
    "from   sklearn.pipeline        import FeatureUnion\n",
    "from   sklearn.ensemble        import RandomForestRegressor\n",
    "import time\n",
    "from   sklearn.metrics         import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "train_df = pd.read_csv('./data1/processed.csv', low_memory=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can encode and scale the data, and apply the regressor we need to split the data into test and training data. To do this I used the StratifiedShuffleSplit function because I wanted the different features to be as truthfully represented, in the training and test data, as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split is then performed based on the most important feature. To find this I used the correlation matrix. Here I look at the highest and the lowest values. For high absolute value the features are highly correlated linearly.\n",
    "\n",
    "I want to find the feature that correlates the most with `Yards` because this is the target feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest:  Yards    1.00000\n",
      "DP_DB    0.08315\n",
      "Name: Yards, dtype: float64\n",
      "smallest: DefendersInTheBox   -0.10709\n",
      "Name: Yards, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = train_df.corr();\n",
    "print(\"largest: \", corr_matrix[\"Yards\"].nlargest(n=2));\n",
    "print(\"smallest:\", corr_matrix[\"Yards\"].nsmallest(n=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix results above we see that the feature that correlates the most with `Yards` is `DefendersInTheBox`, and so we choose to perform the split using this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in split.split(train_df, train_df[\"DefendersInTheBox\"]):\n",
    "    data_train = train_df.loc[train_index];\n",
    "    data_test  = train_df.loc[test_index];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save these training data into separate files so we can easily extract them for testing the encoding methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('./data1/train1.csv', index=False);\n",
    "data_test.to_csv('./data1/test1.csv', index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training data in one file and the test data in another.\n",
    "\n",
    "When training the regressor we don't want to use the target feature, so we need to divide the data even further into `data`, which we will train on, and `labels`, the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df[\"Yards\"].copy();\n",
    "data   = train_df.drop(\"Yards\", axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the RandomForest regressor we have to encode the string features and a standard scaler on the numeric features because of their wide range. The encoding and the scaling are both performed in a pipeline. For the pipelines we also need to create a function that chooses either the numerical or the string features.\n",
    "\n",
    "For this we write the function `DataFrameSelector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attributeNames):\n",
    "        self.attributeNames = attributeNames;\n",
    "    def fit(self, X, y = None):\n",
    "        return self;\n",
    "    def transform(self, X):\n",
    "        return X[self.attributeNames].values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the `DataFrameSelector` we need to separate between the numerical and the string features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = np.array(data.dtypes[(data.dtypes == 'float64') | (data.dtypes == 'int64')].index);\n",
    "str_attributes = np.array(data.dtypes[data.dtypes == 'object'].index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the encoding I chose to use the `OrdinalEncoder`. The features where the values are strings are encoded with integers, and then those integers are converted into binary code. The digits from the binary string are split into separate columns. \n",
    "\n",
    "I then created the pipeline for the string features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(str_attributes)),\n",
    "    ('encoder',  OrdinalEncoder()),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numeric features I used a pipeline with an imputer to fill in the `NaN` values. Here I used the `mean` strategy because of the lack of outliers in the data. Then I used the `StandardScaler` to scale the data due to its wide range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the numeric values:\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector',   DataFrameSelector(num_attributes)),\n",
    "    ('imputer',    SimpleImputer(strategy='mean')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipelines were combined in a full pipeline, which was then applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two pipelines in a full pipeline:\n",
    "full_pipeline = FeatureUnion(transformer_list = [\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('str_pipeline', str_pipeline),\n",
    "]);\n",
    "\n",
    "# Then we fit the data so it is ready for regression:\n",
    "data_prep = full_pipeline.fit_transform(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the data is ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I settled on using a `RandomForestRegressor` because it yielded the best results. I also want to time the training to make sure it doesn't take too much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  118.36870288848877\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time();\n",
    "reg        = RandomForestRegressor(n_estimators=10, random_state=42);\n",
    "reg.fit(data_prep, labels);\n",
    "stop_time  = time.time();\n",
    "print(\"Training time: \", (stop_time - start_time));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I wanted to look at the RMSE for the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.08571512026827019\n"
     ]
    }
   ],
   "source": [
    "rmse_score = np.sqrt(mean_squared_error(labels, reg.predict(data_prep)));\n",
    "print(\"RMSE = \", rmse_score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our regressor, and the results are quite good, we move on to tuning the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TUNING THE MODEL\n",
    "\n",
    "To tune the model we use a validation set. To get this we split the training data into two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42);\n",
    "for train_ind, test_ind in split.split(data_train, data_train[\"DefendersInTheBox\"]):\n",
    "    val_train = data_train.iloc[train_ind];\n",
    "    val_test  = data_train.iloc[test_ind];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the model again, meaning we need to the training part of the validation set into data and labels, and also run it through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = val_train.Yards.copy();\n",
    "val_data   = val_train.drop(\"Yards\", axis=1);\n",
    "val_prep   = full_pipeline.fit_transform(val_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  117.56320977210999\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time();\n",
    "reg        = RandomForestRegressor(n_estimators=10, random_state=42);\n",
    "reg.fit(val_prep, val_labels);\n",
    "stop_time  = time.time();\n",
    "print(\"Training time: \", (stop_time - start_time));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.17211936070041373\n"
     ]
    }
   ],
   "source": [
    "rmse_score = np.sqrt(mean_squared_error(val_labels, reg.predict(val_prep)));\n",
    "print(\"RMSE = \", rmse_score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To hypertune the parameters in the regression I used `RandomizedSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats             import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined the number of iterations `N`, and the parameter range for `n_estimators`, `max_features` and `max_depth`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5;\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(10, 100), #50, 70\n",
    "    \"max_features\": randint(round(val_prep.shape[1]/3), val_prep.shape[1]),\n",
    "    \"max_depth\"   : randint(18,40),\n",
    "    \"random_state\": [42],\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time   = time.time();\n",
    "randomsearch = RandomizedSearchCV(reg, param_distributions=param_dist, n_iter=N, cv=5, iid=False, n_jobs=-1);\n",
    "randomsearch.fit(val_prep, val_labels);\n",
    "stop_time    = time.time();\n",
    "print(\"Search took: \", (stop_time-start_time)/60, \"minutes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best features found through `RandomSearchCV` had an RMSE of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE = \", np.sqrt(mean_squared_error(val_labels, randomsearch.best_estimator_.predict(val_prep))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the best parameters were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 36, max_features = 49, n_estimators = 77, random_state = 42;\n",
    "print(randomsearch.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I extract the best estimator from the `RandomSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_best = randomsearch.best_estimator_;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the best model I can move on to presenting the solution from the test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PRESENTING THE SOLUTION\n",
    "\n",
    "For this part I will load the test data from `/data1/test1.csv` and perform a regression on these data, and examine the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data1/test1.csv', low_memory=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting the test data with the pipeline I ran into an issue with the `DisplayName` category because there are some instances where the player is only in one play. Because these were not sorted into the training set the pipeline doesn't know how to handle them. \n",
    "\n",
    "I would like to figure out a way to fix this, but I just can't figure it out (even after extensive googling), so I just ended up removing these names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "troublemakers = test_data.DisplayName.value_counts()[test_data.DisplayName.value_counts() < 3].index.tolist();\n",
    "inds = [];\n",
    "for name in test_data.DisplayName:\n",
    "    if name in troublemakers:\n",
    "        inds.append(test_data.DisplayName[test_data.DisplayName == name].index[0]);\n",
    "test_data = test_data.drop(inds, axis=0);\n",
    "print(len(troublemakers));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can split the data and perform the regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = test_data[\"Yards\"].copy();\n",
    "data_test  = test_data.drop(\"Yards\", axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I transform the data using the pipeline that has been fitted to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = full_pipeline.transform(data_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_best = joblib.load('./data1/reg_best.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the final RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE:  0.17406330463247444\n"
     ]
    }
   ],
   "source": [
    "final_pred = reg_best.predict(test_prep);\n",
    "print(\"Final RMSE: \", np.sqrt(mean_squared_error(label_test, final_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final results to a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({'Id': data_test.index.values, 'Yards': final_pred})\n",
    "final_df.to_csv(\"./data1/final_results.csv\", index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
